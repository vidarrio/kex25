\documentclass{kththesis}

\usepackage{blindtext} % This is just to get some nonsense text in this template, can be safely removed

\usepackage[
    citestyle=authoryear-comp, 
    bibstyle=authoryear-comp,
    sorting=nty, 
    sortcites=true,
    maxcitenames=1,
    uniquelist=false,
    citetracker=true,
    dashed=false,
    autocite=inline
]{biblatex}

% This sets chronological ordering for citations specifically
\ExecuteBibliographyOptions{
    sortcites=ynt          % Year, name, title order for citations
}

% add comma to citations
\DeclareDelimFormat{nameyeardelim}{\addcomma\space}

\usepackage{csquotes} % Recommended by biblatex
\addbibresource{references.bib} % The file containing our references, in BibTeX format

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{algorithm}
\usepackage{algpseudocode}% Package for algorithms
\usepackage{xcolor}
\usepackage{hyperref}
\hypersetup{
    colorlinks,
    linkcolor={red!50!black},
    citecolor={blue!50!black},
    urlcolor={blue!80!black}
}

\title{Comparative Analysis of A* and Q-Learning Algorithms for Robot Path Planning in Dynamic Warehouse Environments}
\alttitle{Jämförande analys av A* och Q-learning-algoritmer för robotbanplanering i dynamiska lagerlokaler}
\author{Lucas Kristiansson\\Felix Winkelmann}
\email{felixwin@kth.se}
\supervisor{Jana Tumova}
\examiner{Pawel Herman}

\programme{Master in Computer Science}
\school{Royal Institute of Technology}
\date{\today}

% Uncomment the next line to include cover generated at https://intra.kth.se/kth-cover?l=en
% \kthcover{kth-cover.pdf}


\begin{document}

% Frontmatter includes the titlepage, abstracts and table-of-contents
\frontmatter

\titlepage

\begin{abstract}
  English abstract goes here.

  
\end{abstract}


\begin{otherlanguage}{swedish}
  \begin{abstract}
    
  \end{abstract}
\end{otherlanguage}


\tableofcontents


% Mainmatter is where the actual contents of the thesis goes
\mainmatter

%%%%%%%%%%%%%%%%%%%%%%%%%%%---INTRODUCTION---%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Introduction}

The evolution of technology has transformed the way people shop, with e-commerce experiencing rapid growth in recent years \parencite{techtarget}. Customers expect fast delivery, adding pressure to the companies in question. As demand continues to rise, businesses must become more efficient to keep up. One crucial aspect is warehouse management, where optimizing operations is essential. The implementation of warehouse robots can significantly improve efficiency by automating tasks, reducing errors, and accelerating order fulfillment. These robots can be used to pick and process customer orders more quickly and accurately, ensuring faster deliveries and improved customer satisfaction. Additionally, the introduction of robots can help reduce employee costs by minimizing the need for manual labor in warehouses, allowing companies to allocate resources more effectively. 

However, for warehouse robots to operate efficiently, they must be programmed with optimized path-finding algorithms that enable them to navigate warehouse environments quickly and safely. The purpose of this thesis is to investigate how different path-planning algorithms compare in terms of throughput and adaptability in dynamic warehouse environments. Specifically, we focus on evaluating and comparing the effectiveness of the A* algorithm , a well-established heuristic search method known for efficiently finding shortest paths, and the Q-learning algorithm, a reinforcement learning technique capable of adapting to dynamic and unpredictable environments thourh experience-based learning. 

By analyzing the throughput defined as the number of items transported from point A to B of robots utilizing these two algorithms, this research aims to provide insights into their practical suitability for warehouse logistics.


\section{Warehouse}
Order picking is a crucial task in warehouses, directly impacting efficiency and customer satisfaction. Traditionally, manual order picking is performed by humans workers who retrieve items from storage locations and prepare them for shipping. This process involves workers navigating aisles, collecting items, and fulfilling orders based on predetermined lists (orders). Manual order picking is both time-consuming and exhausting. In addition, this process accounts for a significant portion of warehouse costs \parencite{Rene}.

Order picking can be done with different strategies based on the layout of the warehouse and the item selection, the following strategies are common \parencite{CognitOps}.

\textbf{Piece Picking:}
This type of picking involves picking each item individually from an order. This picking type is used by e.g. E-commerce businesses like Amazon because the orders are often small and a mix of products.

\textbf{Batch Picking:}
This type of picking involves grouping multiple orders with common items, it is then possible to pick more than one object per storage destination. This minimizes the travel distance since multiple orders will be picked simultaneously. This picking type is used by e.g. supermarkets for online grocery orders, where employees gather common items for multiple customers simultaneously.

\textbf{Zone Picking:}
This picking type involves dividing the warehouse into multiple zones and limiting the pickers to specific zones. These pickers will pick items in their own zone, and then the picked items are consolidated into an order. This picking style is used by E.g. Auto parts warehouses, as it effectively divides items into zones based on size, creating a streamlined picking strategy.

No matter the picking strategy a warehouse employs, human error is always a possibility when people are involved—such as selecting the wrong items or quantities. Additionally, as previously mentioned, order picking can be physically demanding, requiring workers to walk long distances and lift heavy items. Moreover, it is a major expense, accounting for approximately 65\% of total warehouse costs. To address these challenges, many companies have invested in robots to handle this costly process, reducing expenses while also protecting the health of their workers \parencite{Hung-Yu}.

\section{Robots}
There are several ways robots can be used in warehouses to enhance the efficiency of order picking.

One approach is the Basket System, where automated vehicles carry a pick cart that follows the human worker throughout the picking process. Acting as a mobile basket, the robot moves alongside the worker, collecting items as they are picked. Once the order is complete, the robot autonomously transports the basket to a delivery station, while a new robot with an empty basket takes its place. This method optimizes the worker’s walking path, minimizing unnecessary movement and increasing efficiency \parencite{Koster}.

Another strategy is the Robotic Movable Rack (RMR) System, where robots transport entire storage racks containing multiple items. When an item is requested, a robot retrieves the rack holding the item and brings it to a stationary human worker, who then picks the required product and places it into an order bin. In this setup, the human remains in one place while the robots handle all transportation, significantly reducing physical strain and improving productivity \parencite{Koster}.

Some robots are capable of handling the entire order-picking process independently. They can navigate the warehouse, pick items into a basket, and transport the completed order to its delivery station. By automating these tasks, these robots can effectively replace human pickers, improving efficiency and reducing labor costs \parencite{BrightPick}.

\section{Research Question}
This thesis aims to answer the following research question:
\\
How do the A* algorithm and Q-learning algorithm compare in terms of throughput, defined as the number of items transported per unit of time when utilized by robots for path planning in a dynamic warehouse environment?

\section{Scope and Limitations}
This thesis specifically investigates the comparative performances of A* and Q-learning algorithms in a simulated environment that replicate real-world warehouse conditions. The primary metric for comparison is throughput, focusing on efficiency in navigating and completing transportation tasks. 
\\
This thesis does not address the following aspects:
\\
\begin{itemize}
    \item Economic analyses such as cost-benefit calculations or ROI assessments.
    
    \item Detailed mechanical or hardware-related performance concerns, such as energy consumption, robot reliability, or maintenance needs.

    \item Real-world experimental validation or deployment in operational warehouses. 

\end{itemize}
These limitations define clear boundaries for this research, ensuring a focused exploration of algorithmic performance within controlled simulation scenarios.


%%%%%%%%%%%%%%%%%%%%%%%%%%%---BACKGROUND---%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Background}
\section{Algorithms}

A fundamental aspect of robotics is path planning. Path planning algorithms determine the optimal route for a robot to navigate from its starting point to its destination. There are several path planning algorithms, each with its own strengths and weaknesses. This section provides a brief overview of common path planning algorithms, followed by a detailed discussion of A* and Q-learning, which are the focus of this study.

\subsection{Common Path Planning Algorithms}

\textbf{A* Algorithm:}
The A* algorithm is a popular path planning algorithm that uses a heuristic to estimate the cost of reaching the goal from a given point. It is widely used in robotics due to its efficiency and accuracy in finding the shortest path. The A* algorithm is based on Dijkstra's algorithm but incorporates a heuristic to guide the search process, making it more efficient \parencite{Hart68}. 

\textbf{RRT:}
The Rapidly-exploring Random Tree (RRT) algorithm is a probabilistic path planning algorithm that is well-suited for high-dimensional spaces. It works by incrementally building a tree of random samples from the configuration space, connecting them to the nearest node in the tree. The RRT algorithm is particularly useful for robots with complex kinematics or environments with obstacles, as it can efficiently explore the space and find feasible paths \parencite{lavalle1998rapidly}.

\textbf{D* Lite:}
The D* Lite algorithm is an incremental path planning algorithm that is well-suited for dynamic environments. It works by updating the path incrementally as new information becomes available, allowing the robot to adapt to changing conditions. The D* Lite algorithm is particularly useful for robots operating in environments with moving obstacles or changing terrain, as it can quickly update the path to avoid collisions and reach the goal efficiently \parencite{Koenig02}.

\textbf{Swarm Intelligence:}
Swarm intelligence algorithms are inspired by the collective behavior of social insects, such as ants and bees. These algorithms work by simulating the interactions between individual agents to find optimal solutions to complex problems. Swarm intelligence algorithms are particularly useful for multi-robot systems, as they can coordinate the actions of multiple robots to achieve a common goal. Some common swarm intelligence algorithms include Ant Colony Optimization (ACO), which mimics the foraging behavior of ants, and Particle Swarm Optimization (PSO), which simulates the social behavior of birds flocking or fish schooling \parencite{Dorigo04,Kennedy95}.

\textbf{Reinforcement Learning:}
Reinforcement learning is a machine learning technique that enables robots to learn optimal policies through trial and error. By interacting with the environment and receiving feedback on their actions, robots can learn to navigate complex spaces and perform tasks efficiently. Reinforcement learning is particularly useful for robots operating in dynamic environments, as they can adapt their behavior based on changing conditions. Common reinforcement learning algorithms include value-based methods like Q-learning and Deep Q Networks (DQN), as well as policy-based methods like Advantage Actor-Critic (A2C) and Proximal Policy Optimization (PPO). Each approach has different strengths and limitations in terms of stability, sample efficiency, and computational requirements \parencite{Sutton18, Schulman17}.


\subsection{Centralized vs. Decentralized Algorithms}

\textbf{Centralized Algorithms:}
Centralized algorithms involve a single centralized entity making decisions for all robots in the system. This approach is simple and efficient, as the centralized entity can coordinate the actions of all robots to achieve a common goal. However, this approach can be vulnerable to single points of failure, as the centralized entity may become a bottleneck if it is overloaded or fails. In addition, centralized algorithms may not scale well to large numbers of robots, as the centralized entity may struggle to coordinate the actions of all robots effectively.

\textbf{Decentralized Algorithms:}
Decentralized algorithms involve individual robots making decisions autonomously based on local information. This approach is more robust and scalable than centralized algorithms, as each robot can make decisions independently without relying on a centralized entity. Decentralized algorithms are well-suited for multi-robot systems, as they can adapt to changing conditions and operate effectively in dynamic environments. However, decentralized algorithms may be less efficient than centralized algorithms, as robots may not coordinate their actions optimally to achieve a common goal.

\subsection{Centralized Training with Decentralized Execution (CTDE)}
Centralized Training with Decentralized Execution (CTDE) is a hybrid approach that combines the benefits of centralized and decentralized algorithms. In CTDE, a centralized entity trains the robots in the system using a global model, whereafter the robots execute their action autonomously based on local information. This gives the robots the flexibility to adapt to changing conditions while still benefiting from the more efficient coordination provided by centralized training. CTDE is well-suited for multi-robot systems, as it can leverage the advantages of both centralized and decentralized algorithms to achieve optimal performance \parencite{CTDE17}.

\textbf{Application in this study:}
In this study, we apply CTDE to both A* and Q-learning algorithms, since the warehouse environment is a dynamic and complex space that requires robots to adapt to changing conditions while still coordinating their actions effectively. Therefore, we have the following:

\begin{itemize}
    \item A* with CTDE: Centralized computation of paths, decentralized execution with local adjustments.
    \item Q-learning with CTDE: Centralized learning from shared experiences, decentralized decision-making based on learned policies.

\end{itemize}

\subsection{A* Algorithm}

\textbf{Overview:}
The A* algorithm is the best-first search algorithm that finds the shortest path from a start node to a goal node in a weighted graph. It was first described by Peter Hart, Nils Nilsson, and Bertram Raphael in 1968 \parencite{Hart68}. A* combines the strengths of Dijkstra's algorithm (completeness and optimality) with the efficiency of a heuristics-guided approach. This makes it particularly well-suited for path planning in our warehouse environment, where the robots need to navigate efficiently between storage locations and around obstacles to pick items and fulfill orders.

\textbf{A* detailed explanation:}
The core principle of the A* algorithm is to minimize the total estimated cost from the start node to the goal node. For each node $n$ in the graph, A* maintains two key values:

\begin{itemize}
    \item $g(n)$: The actual cost of the path from the start node to node $n$.
    \item $h(n)$: The heuristic function that estimates the cost of the path from node $n$ to the goal node.
\end{itemize}

The total estimated cost, denotes as $f(n)$, is calculated as:

\begin{equation}
    f(n) = g(n) + h(n)
\end{equation}

The A* algorithm uses a priority queue to explore nodes in order of increasing $f(n)$ value. At each step, the algorithm selects the node with the lowest $f(n)$ value and expands it by considering its neighbors. The algorithm terminates when the goal node is reached or when the priority queue is empty.

It is described as seen in algorithm \ref{Astar}.

\begin{algorithm}
\caption{A* Algorithm}
\label{Astar}
\begin{algorithmic}[1]
\State Initialize openSet with the start node
\State Initialize closedSet as empty
\State For the start node, set $g(start) = 0$
\State For the start node, set $f(start) = g(start) + h(start)$ 
\While{openSet is not empty}
    \State current = node in openSet with lowest $f$ value
    \If{current is the goal node}
        \State Reconstruct path and return success
    \EndIf
    \State Remove current from openSet
    \State Add current to closedSet
    \For{each neighbor of current}
        \If{neighbor in closedSet}
            \State Continue to next neighbor
        \EndIf
        \State tentativeG = $g(current) + cost(current, neighbor)$
        \If{neighbor not in openSet}
            \State Add neighbor to openSet
        \ElsIf{tentativeG $>=$ $g(neighbor)$}
            \State Continue to next neighbor
        \EndIf
        \State $parent(neighbor) = current$
        \State $g(neighbor) = tentativeG$
        \State $f(neighbor) = g(neighbor) + h(neighbor)$
    \EndFor
\EndWhile
\State Return failure (no path found)
\end{algorithmic}
\end{algorithm}

The heuristic function $h(n)$ is crucial to the performance of the A* algorithm. It provides an estimate of the cost of the path from node $n$ to the goal node. Common heuristics in grid-based environments include:

\begin{itemize}
    \item Manhattan distance: $h(n) = |x_n - x_{goal}| + |y_n - y_{goal}|$
    \item Euclidean distance: $h(n) = \sqrt{(x_n - x_{goal})^2 + (y_n - y_{goal})^2}$
    \item Chebyshev distance: $h(n) = \max(|x_n - x_{goal}|, |y_n - y_{goal}|)$ 
\end{itemize}

For a heuristic to guarantee that A* finds the optimal path, it must be admissible, meaning it never overestimates the actual cost to reach the goal. Additionally, if the heuristic is consistent (or monotonic), meaning that for any node $n$ and its neighbor $m$, $h(n) \leq c(n, m) + h(m)$, where $c(n, m)$ is the cost of moving from node $n$ to node $m$, then A* is guaranteed to find the optimal path with the fewest number of expansions \parencite{Hart68}.

\textbf{A* with CTDE:}
In our CTDE approach to A*, the algorithm is applied as follow:

\begin{itemize}
    \item \textbf{Centralized Training:} A centralized entity computes the optimal paths for all robots simultaneously, using a global model of the warehouse environment, including the locations of storage locations, obstacles, and other robots. This allows for global path optimization and conflict resolution, ensuring that the robots can navigate efficiently and avoid collisions.
    \item \textbf{Decentralized Execution:} Each robot independently follows its assigned path but maintains the ability to make local adjustments based on its immediate surroundings. This means they can make limited local deviations to the path to avoid obstacles or other robots, while still following the overall plan. 
\end{itemize}

By implementing A* with CTDE, we can leverage the efficiency of centralized path planning while still allowing for decentralized execution, enabling the robots to adapt to changing conditions and operate effectively in a dynamic warehouse environment. The central system can intermittently update the paths based on new information, ensuring that the robots always have the most up-to-date plans to follow.

\subsection{Q-learning Algorithm}

\textbf{Overview:}
Q-learning is a model-free reinforcement learning algorithm that enables agents to learn optimal action policies through trial and error. It was first introduced by Chris Watkins in 1989 \parencite{Watkins89}. It has since then become a foundational algorithm in the field of reinforcement learning, known for its simplicity and effectiveness in solving a wide range of problems. In our warehouse environment, Q-learning can be used to train robots to adapt to changing conditions and learn optimal policies for navigating the warehouse, picking items, and fulfilling orders.

\textbf{Comparison with other RL algorithms:}
Add info about other RL algorithms (DQN, PPO, A2C, model-based) and what we've chosen for this study.

\textbf{Reinforcement Learning Basics:}
Reinforcement learning is a type of machine learning that enables agents to learn optimal policies by interacting with an environment and receiving rewards or penalties based on their actions. The goal of reinforcement learning is to maximize the cumulative reward over time by learning the optimal policy that maps states to actions. In the context of our warehouse environment, the robots are the agents, the warehouse is the environment, and the rewards are based on the efficiency of the robots' actions, such as picking items and fulfilling orders.

Reinforcement learning is framed as a Markov Decision Process (MDP), defined by:

\begin{itemize}
    \item States $S$: The set of all possible states the agent can be in.
    \item Actions $A$: The set of all possible actions the agent can take.
    \item Transition function $P(n'|n,a)$: The probability of transitioning from one state to another given an action.
    \item Reward function $R(n,a,n')$: The reward received by the agent for taking an action in a given state.
    \item A discount factor $\gamma \in [0,1]$: Determines the importance of future rewards. 
\end{itemize}

The goal of the agent is to learn a policy $\pi: S \rightarrow A$ that maximized the expected cumulative discounted reward:

\begin{equation}
    E\left[\sum_{t=0}^{\infty} \gamma^t R(s_t, a_t, s_{t+1})\right]
\end{equation}

\textbf{Q-learning detailed explanation:}
The Q-learning algorithm centers around updating a Q-table that stores the esimated values of all state-action pairs. The Q-value of a state-action pair $(s, a)$ represents the expected cumulative reward of taking action $a$ in state $s$ and following the optimal policy thereafter. The Q-value is updated iteratively using the Bellman equation:

\begin{equation}
    Q(s, a) \leftarrow Q(s, a) + \alpha \left[R(s, a) + \gamma \max_{a'} Q(s', a') - Q(s, a)\right]
\end{equation}

Where:

\begin{itemize}
    \item $\alpha$ is the learning rate, determining the impact of new information on the Q-value.
    \item $R(s, a)$ is the immediate reward of taking action $a$ in state $s$.
    \item $\gamma$ is the discount factor, determining the importance of future rewards.
    \item $s'$ is the next state after taking action $a$ in state $s$.
    \item $\max_{a'} Q(s', a')$ is the maximum Q-value of all possible actions in state $s'$.
\end{itemize}

The Q-learning algorithm iteratively updates the Q-values based on the rewards received by the agent, gradually learning the optimal policy for navigating the environment and maximizing the cumulative reward. The algorithm continues to explore the environment while exploiting the learned policy, balancing between exploration and exploitation to find the optimal policy.

As pseudo code, the Q-learning algorithm can be described as seen in algorithm \ref{QL}.

\begin{algorithm}
\caption{Q-learning Algorithm}
\label{QL}
\begin{algorithmic}[1]
\State Initialize Q(s,a) arbitrarily for all $s \in S$, $a \in A$
\State Set hyperparameters: learning rate $\alpha \in [0,1]$ and discount factor $\gamma \in [0,1]$
\For{each episode}
    \State Initialize state $s$
    \While{$s$ is not terminal}
        \State Choose action $a$ from $s$ using policy derived from Q (e.g., $\epsilon$-greedy)
        \State Take action $a$, observe reward $r$ and next state $s'$
        \State Update\hspace{0.1cm}Q-value: 
        \State $Q(s, a) \leftarrow Q(s, a) + \alpha \left[R(s, a) + \gamma \max_{a'} Q(s', a') - Q(s, a)\right]$
        \State $s \leftarrow s'$
    \EndWhile
\EndFor
\end{algorithmic}
\end{algorithm}

\textbf{Scalability Challenges in Tabular Q-learning:}
In a warehouse environment, even a modest 34×32 grid combined with dynamic elements like human workers, other robots, and task-specific states (e.g., carrying status and goal locations) creates a state space so large that storing a traditional Q-table becomes impossible. The number of possible states grows exponentially with each added variable, a phenomenon known as the curse of dimensionality. This explosion isn't just about memory; it means the robot would need to visit each state-action pair an impractical number of times to learn anything useful.

\textbf{Deep Q-learning (DQN) as a solution:}
The solution is to replace the Q-table with a neural network (a Deep Q-Network or DQN) that learns to approximate Q-values \parencite{mnih2013playing,mnih2015human}. Instead of storing every possible $Q(s,a)$ combination, the network $Q(s,a;\theta)$ with weights $\theta$ takes our 10×5×5 observation tensor, encoding the robot's local surroundings, goals, and status, and outputs Q-values for all possible actions. The network isn't just a lookup table; it learns to generalize from similar states, making the problem tractable. We can express this approximation of the optimal action-value function mathematically as:

\begin{equation}
    Q^*(s,a) = \max_{\pi} \mathbb{E} \left[ r_t + \gamma r_{t+1} + \gamma^2 r_{t+2} + \ldots | s_t = s, a_t = a \right]
\end{equation}

which is the maximum sum of rewards $r_t$ discounted by $\gamma$ at each time step $t$, achievable by a policy $\pi$ = $P(a|s)$, after making an observation $s$ and taking an action $a$.

We train this network by minimizing the difference between its predictions and the ideal target Q-values, which combine the immediate reward with the discounted future rewards the robot can expect. Mathematically, we express this as minimizing the Bellman error:

\begin{equation}
    \mathcal{L}(\theta) = \mathbb{E}_{(s,a,r,s') \sim \mathcal{D}} \left[ \big(r + \gamma \max_{a'} Q(s',a';\bar{\theta}) - Q(s,a;\theta)\big)^2 \right]
\end{equation}

Here, $\bar{\theta}$ represents a separate target network whose weights are periodically synchronized with the main network. This separation stabilizes training by preventing the network from chasing its own tail, updating its predictions against moving targets. However, this approach suffers from instability due to correlated updates and moving targets.

\textbf{Stabilizing DQNs with Experience Replay:}
If we trained the network on consecutive experiences as the robot moves through the warehouse, the strong correlations between nearby states would destabilize learning. Instead, we store all experiences $(s,a,r,s')$ in a replay buffer $\mathcal{D}$, then sample them randomly in mini-batches. This is akin to how humans learn better from shuffled flashcards rather than reading a textbook line by line. Some experiences are more informative than others. When the network's prediction is wildly wrong (high temporal difference error $\delta_i$), that transition deserves more attention:

\begin{equation}
    p_i = |\delta_i| + \epsilon, \quad \text{where} \quad \delta_i = r + \gamma \max_{a'} Q(s',a';\bar{\theta}) - Q(s,a;\theta)
\end{equation}

By sampling these high-error experiences more frequently, we accelerate learning, much like how focusing on difficult problems improves our understanding faster than reviewing what we already know. 


\textbf{The Overestimation Problem and DDQN:}
A subtle issue arises because the same network both selects and evaluates actions through the $\max$ operator. This leads to systematically overoptimistic Q-values. Double DQN (DDQN) fixes this by decoupling the two steps:

\begin{enumerate}
    \item Use the main network $\theta$ to select the best action for the next state: $a^* = \arg\max_{a'} Q(s',a';\theta)$
    \item Use the target network $\bar{\theta}$ to evaluate the action's value: $Q(s',a^*;\bar{\theta})$
\end{enumerate}

This simple change results in more accurate value estimates and better-performing policies.

\textbf{Exploration vs. Exploitation:}
Early in training, the robot must explore randomly to discover rewarding behaviors. Later, it should exploit what it's learned. The $\epsilon$-greedy policy smoothly manages this transition:

\begin{equation}
    a = \begin{cases} 
        \text{random action} & \text{with probability } \epsilon_t \\
        \arg\max_{a} Q(s,a;\theta) & \text{otherwise}
    \end{cases}
\end{equation}

where $\epsilon_{t}$ decays over time, typically from 1.0 (pure exploration) to 0.01 or lower (mostly exploitation). This mimics how humans gradually shift from broad experimentation to refined expertise.

\textbf{Q-learning with CTDE:}
In our CTDE approach to Q-learning, the algorithm is applied as follow:

\begin{itemize}
    \item \textbf{Centralized Training:} During the training phase, all robots share their experiences (state transitions, actions, rewards) with a centralized learning system. This centralized learning process has several advantages. First, it allows the robots to learn from each other's experiences, accelerating the learning process and improving the overall performance. Second, it can detect conflicts between robots policies and resolve them to ensure that all robots can operate effectively in the warehouse environment. Third, it enables emergent coordination patterns to appear, as the robots learn to work together to achieve common goals. Global rewards can be incorporated to encourage cooperation and discourage selfish behavior.
    \item \textbf{Decentralized Execution:} Once trained, each robot executes its learned policy independently, selecting actions based on its local observations and the learned Q-values. This method allows the robots to adapt to changing conditions and operate effectively. It's more robust to failures and communication delays, as each robot can make decisions autonomously without relying on a centralized entity, and it has great scalability, as new robots can be added to the system without requiring changes to the centralized learning process.
\end{itemize}

By implementing Q-learning with CTDE, we can leverage the benefits of centralized training and decentralized execution to train robots to navigate the warehouse, pick items, and fulfill orders efficiently. The centralized learning process ensures that the robots can learn from each other's experiences and coordinate their actions effectively, picking up complex traffic patterns, while the decentralized execution allows the robots to adapt to changing conditions and operate autonomously in the warehouse environment.


\section{Related work}
Research on path planning for multiple robots in warehouse environments has explored both classical algorithms and reinforcement learning (RL). A* and its variants are commonly used for finding efficient paths in grid-based layouts and are known for producing collision-free and near-optimal results. However, these methods can struggle under high traffic or dynamic conditions due to increased computational demands. Some studies have modified A* to avoid conflicts over time or optimized it for handling online path requests, but purely heuristic approaches tend to perform worse in more complex, real-time scenarios.

To overcome these limitations, many recent works have turned to RL. One direction has been to compare A* with deep Q-network (DQN) policies in single-agent tasks. For example, \cite{Li} showed that DQN agents trained to navigate cluttered warehouse environments outperformed A* in terms of task success rate, accuracy, and F1 score. They also introduced a hybrid method called Proximal Policy-Dijkstra (PP-D), which combined RL with classical planning and achieved even better results. However, their evaluation focused on single-agent navigation rather than throughput in a multi-agent system, and they did not use centralized training with decentralized execution (CTDE).

Other recent studies, such as \cite{wang}, combine multi-agent RL with large neighborhood search to improve coordination. Their hybrid algorithm used RL in early stages to resolve collisions, then switched to prioritized A* to complete the remaining paths. The method achieved high success rates in dense scenarios where classical solvers failed. However, this approach differs from ours, which distinctly compares pure implementations of A* and DQN without blending the methods, thereby isolating their individual strengths and weaknesses.

In summary, previous work shows that RL-based planners can outperform classical algorithms like A* under certain conditions, especially in dynamic and multi-agent settings. Hybrid methods have also demonstrated improved performance by combining the strengths of both approaches. Our thesis builds on these findings by providing a direct comparison between a classical planner and a learned policy in a shared environment, focusing specifically on throughput. Unlike most prior studies, we use a CTDE framework and the PettingZoo multi-agent environment to evaluate how learning-based methods affect efficiency in a realistic warehouse setting.


%%%%%%%%%%%%%%%%%%%%%%%%%%%---METHODS---%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Methods}

\section{Overview}
The goal of comparing A* and Q-learning in this thesis is to determine which algorithm more effectively optimizes robot path planning in dynamic warehouse environments. The primary metric evaluated is throughput, the number of items successfully transported per unit of time. Secondary metrics include collision avoidance efficiency and adaptability to dynamic obstacles. 

\section{Environment Setup}
In order to simulate a realistic warehouse environment for evaluating pathfinding algorithms, we developed a custom multi-agent simulation using the PettingZoo library, which is a Python library designed specifically for reinforcement learning in multi-agent systems. The PettingZoo library provides standardized interfaces for interacting with multi-agent environments and supports several API styles. In our case, we chose the Parallel API, implemented by subclassing pettingzoo.utils.ParallelEnv.


\subsection{PettingZoo}
PettingZoo's ParallelEnv interface allows multiple agents to act simultaneously at each step, which is crucial in our case where multiple warehouse robots and humans operate concurrently. Implementing a custom environment with this API requires defining several core methods:

\begin{itemize}
    \item \texttt{\_\_init\_\_()} initializes the environment's configuration and parameters.

    \item \texttt{reset()} resets the entire environment to its initial state, returning initial observations and info about the robots.

    \item \texttt{step()} Advances the environment one step based on a dictionary of actions (one per agent). This method updates agent and human positions, handles collisions, applies rewards, and generates new observations

    \item \texttt{render()} visualizes the current state of the environment.

    \item \texttt{action\_space(agent)} defines every action a robot can take.

    \item \texttt{observation\_space(agent)} defines the observation space that a robot currently has.
\end{itemize}

Each agent receives a local observation in the form of a \texttt{(10, 5, 5)} tensor, representing a 5\texttimes5 grid centered around the agent. Each of the 10 channels in this tensor contains binary values (0 or 1) and encodes the following information:

\begin{enumerate}
\item \textbf{Agent position}: A 1 is placed at the center cell to indicate the agent's own position.
\item \textbf{Other agents}: Positions of other agents within the 5\texttimes5 window are marked with 1s.
\item \textbf{Shelves (static obstacles)}: Cells that contain shelf blocks are marked with 1s.
\item \textbf{Humans (dynamic obstacles)}: Cells occupied by human workers are marked with 1s.
\item \textbf{Current goal}: If the agent's current goal (either a pickup or dropoff point) is visible within the window, it is marked with a 1.
\item \textbf{Pickup points}: All visible pickup points within the local observation area are marked with 1s.
\item \textbf{Dropoff points}: Dropoff points in the 5\texttimes5 area are indicated by 1s.
\item \textbf{Carrying status}: If the agent is currently carrying an item, the entire channel is filled with 1s; otherwise, it contains only 0s.
\item \textbf{Valid pickup/dropoff indicator}: This channel highlights whether the agent is currently at a valid pickup or dropoff location based on its task progress. For example, if an agent that is carrying an item is standing on the correct dropoff cell, that cell is marked with a 1.
\item \textbf{Goal Compass}: This channel tells the agent in which direction the goal is located. 
\end{enumerate}

This observation structure ensures agents receive all relevant information about their local surroundings, including task-relevant context and dynamic obstacles, while keeping the state representation compact and scalable. The observations space is illustrated in Figure \ref{Observation-space}.

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{observation-space.png}
    \caption{Observation-space (5x5 x 10)}
    \label{Observation-space}
\end{figure}


\subsection{Environment Layout}
The warehouse is modeled as a two-dimensional grid. The grid size is adjustable depending on the scenario, but in our case we used a default size of 34×32 cells. Shelves are implemented as \textbf{static obstacles} in this grid, and their arrangement is designed to resemble a realistic warehouse setup with evenly spaced aisles to support agent navigation.
\\
Each shelf block consists of: 

\begin{itemize} 
    \item 2 rows in height (vertical size) 
    
    \item 4 columns in width (horizontal size) 
    
\end{itemize} 
This means every shelf occupies a 2×4 rectangular area in the grid.
The shelves are placed in a repeating pattern throughout the grid, with 2-cell spacing between shelf blocks both vertically and horizontally. This creates clear aisles for agents to move through. As shown in Figure \ref{Warehouse grid (size 34x32)}, the result is a structured and consistent layout that resembles warehouse shelving rows. Additionally, at least a 2-cell-wide empty border surrounds the grid to give agents space to maneuver around the edges.
This kind of layout introduces realistic constraints that make the path planning problem more challenging. Agents must find efficient routes to navigate around shelves, avoid collisions, and reach pickup and dropoff points — which is exactly the kind of scenario we wanted to simulate for comparing A* and Q-learning.

\begin{figure}
    \centering
    \includegraphics[width=0.5\linewidth]{grid_picture.png}
    \caption{Warehouse grid (size 34x32)}
    \label{Warehouse grid (size 34x32)}
\end{figure}

Shelves are placed in the grid using a straightforward block-based approach. Starting from the top-left corner, shelf blocks of size 2×4 are added row by row, with spacing in between to form aisles. The placement continues until the specified number of shelf cells has been reached, resulting in a structured and navigable warehouse layout.

In the warehouse environment, human workers are modeled as \textbf{dynamic obstacles} that move independently of the robot agents. Their navigation is controlled by a greedy heuristic algorithm that moves each human toward a randomly assigned goal. This adds dynamic complexity to the environment and forces the robot agents to adapt to changing obstacles. Each human is assigned a random goal location on the grid, and once the human reaches the goal, a new goal is automatically assigned. To create more realistic and predictable movement patterns, we have also implemented a feature that allows us to restrict the area in which humans can move. This makes it possible to simulate localized worker activity (e.g., a humans only walking in a certain warehouse zone), which may give the learning-based robots the opportunity to identify and exploit these patterns. 

The human path planning algorithm determines the next move for each human agent in the warehouse simulation. For each human, the algorithm retrieves its current position and goal. If the movement history is uninitialized, it is set up; if a cycle is detected (i.e., the human repeatedly revisits the same positions), a new random goal is assigned and the history is reset. The algorithm then computes the preferred movement by comparing the horizontal and vertical distances to the goal and selects a primary and secondary action accordingly. Crucially, if the selected action is blocked by a robot (as indicated by the \texttt{is\_valid} function returning a blocking signal), the human is set to wait. This ensures that the robot must adjust its path, rather than the human. The pseudocode \ref{Human Path Planning Algorithm} summarizes the process.

\begin{algorithm}
\caption{Human Path Planning Algorithm}
\label{Human Path Planning Algorithm}
\begin{algorithmic}[1]
\For{each human in Humans}
    \State $current \gets$ current position; $goal \gets$ current goal
    \If{history not initialized}
        \State Initialize history
    \EndIf
    \If{cycle detected in history}
        \State Assign new random goal; reset history
    \EndIf
    \State $action \gets$ \Call{ComputeAction}{$current,\, goal$}
    \State Update history with $current$
\EndFor
\State \Return mapping of humans to actions
\Function{ComputeAction}{$current,\, goal$}
    \If{$current = goal$}
        \State Assign new random goal
    \EndIf
    \State $dx \gets (goal_{\text{col}} - current_{\text{col}})$
    \State $dy \gets (goal_{\text{row}} - current_{\text{row}})$
    \If{$\lvert dx\rvert > \lvert dy\rvert$}
        \State $primary \gets$ (\texttt{RIGHT} if $dx > 0$, else \texttt{LEFT})
        \State $secondary \gets$ (\texttt{UP} if $dy < 0$, else \texttt{DOWN})
    \Else
        \State $primary \gets$ (\texttt{UP} if $dy < 0$, else \texttt{DOWN})
        \State $secondary \gets$ (\texttt{RIGHT} if $dx > 0$, else \texttt{LEFT})
    \EndIf
    \If{$primary$ is valid and avoids backtracking}
        \State \Return $primary$
    \ElsIf{$primary$ is blocked by a robot}
        \State \Return \texttt{WAIT}
    \ElsIf{$secondary$ is valid and avoids backtracking}
        \State \Return $secondary$
    \ElsIf{$secondary$ is blocked by a robot}
        \State \Return \texttt{WAIT}
    \Else
        \State \Return first alternative valid action, or \texttt{WAIT} if none
    \EndIf
\EndFunction
\end{algorithmic}
\end{algorithm}



%%%%%%%%%%%%%%%%%%%%%%%%%%%---A* IMPLEMENTAION---%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{A* implementation}
Our A* path planning module is implemented as a centralized manner that plans routes for all the robots in the warehouse, following the CTDE (Centralized Training, Decentralized Execution) paradigm discussed earlier. This planner has full access to the global grid map and the state of all agents, allowing it to compute collision-free paths for every robot simultaneously. each tobot then executes its assigned path step-by-step in a decentralized manner, using local observations to adjust for any unexpected obstacles. This integration ensures that path planning is aware of the wahouse layout and dynamic elements, while each robot can still react autonomously to unexpected obstacles (like dynamic obstacles) without waiting for a global replan.

\textbf{Multi-Agent Coordination}: Planning for multiple robots simulanteasly requires careful avoidance of collision. Our A* agent handles this by performing \textbf{prioritized planning} and introducing the concept of \textbf{space-time reservations}. At the start of each planning cycle, the agent determines an order in which to plan paths for all robots. For fairness and efficiency, we prioritize any robot that is currently carrying an item (since it is on its route to drop-off and should not be delayed), followed by robots that are active on their route to a goal, and lastly any idle or already-at-goal robots. We then plan paths, one agent at a time on that priority order. Once a robot's oath is found, its trajectory is added to a reservation table that marks each cell as occupied at the specific time steps the robot will be there. When planning for the next robots, the A* search treats those reserved space-time coordinates as blocked: if a neighbour state is already reserved by another agent at time t, out algorithm will not move a second robot into that cell at time t. This prevents two robots from colliding by arriving at the same location simultaneously. We also explicitly check and avoid \textbf{swap collision}, where two robots might attempt to move into each other's positions in the same step. In such cases, the planner will require one of them to wait instead of swapping places. To facilitate waiting, the A* state expansion includes the option for a robot to remain in its current cell for one timestep (a wait action) if moving forward is not possible without conflict. These waits are integrated into the search as additional neighbour states and are assigned a slightly higher cost to ensure the algorithm only uses waiting when necessary. After computing all paths, the reservation table holds a schedule plan for each agent's movements. 

\textbf{Dynamic Obstacles and Adaptions}: The warehouse environment is dynamic, with human workers that act as unpredictable obstacles. Our A* implementation accounts for these by combining global planning with real-time local adjustments. The initial path planning primarily considers \textbf{static obstacles} (the immovable shelves, treated as permanent blocks on the grid) and other robots (via reservations). Human workers are not directly included in the global plan since they can move, but the robots are equipped to sense and react to them during execution. Each robot receives a local 5x5 grid observation centered on itself every timestep, which indicates nearby humans and other agents. If a robot's next planned move is towards a cell that a human just stepped into (or any unforeseen obstacle appears), the robot can override the plan momentarily. In our system, the A* agent's execution loop includes a \textbf{local conflict detector} that compares each robot's immediate next planned position with its live observation if a conflict is detected for example, an obstacle r another agent is now occupying the target cell the agent will adjust the plan on the fly. It attempts to find an alternative direction to go around the obstacle just for that step, or inserts a short wait action if no detour is possible. This decentralized execution adjustment does not require re-planning the entire path, the robot will simply resume following the original path (which is still valid beyond the point of conflict) once the local obstacle clears. These on-the-fly adjustments are crucial for dealing with the uncertainty of a dynamic warehouse. Additionally, the planner monitors for any \textbf{deadlock situation}. If a robot gets stuck waiting too long, the system will trigger a global re-plan. For instance, if any agent has been executing a wait action for more than a fixed threshold of consecutive steps, it is marked for re-routing. The next time step, or when the opportunity arises, the A* agent will recompute paths for all robots, treating the current positions as new start states (this often resolves deadlocks by choosing new routes or ordering). We also reser and re-plan paths whenever a robot finishes a task. For example, after a successful pickup or dropoff, the environment assigned a new goal to that robot. Upon such events, or any time an agent's goal changes, the centralized planner is invoked again to find a fresh path to the new goal. In summary, the A* implementation continuously alternates between planning (when needed) and execution, ensuring that each robot always has an up-to-date, safe path towards its goal, while remaining flexible to handle dynamic changes in the environment. 

Overall, this A* planner is highly tailored to the multi-agent warehouse setting. It combines optimality and efficient of the classic A* algorithm (with simple heuristic and effective data structures) with several adaptations for multi-robot coordination and dynamic obstacle avoidance. By integrating a time dimension into the search and using reservations, it successfully schedules multiple robots without collisions. Meanwhile, the CTDE strategy allows the system to react to moving obstacles and new tasks in real time. This implementation provides a strong baseline for compaction against the learning-based approach (Q-learning), as it represents a robust classical solution to the warehouse navigation and task fulfillment problem. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%---Q-LEARNING IMPLEMENTATION---%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Q-learning implementation}


%%%%%%%%%%%%%%%%%%%%%%%%%%%---RESULTS---%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Results}
\section{A* results}
This chapter presents the outcomes of simulations conducted to evaluate the performance of the A* algorithm under various warehouse scenarios. Each robot agent was assigned random pickup points within the warehouse grid and tasked with delivering items to predefined dropoff points located at the corners of the grid. The exact locations of these dropoff points are illustrated in figure \ref{grid-with-dropoff-points}. To ensure consistency and fairness in the evaluation, fixed random seeds were employed across all simulation. This approach guarantees identical starting conditions for the robots and comparable dynamic obstacle behavior between tests. 

\begin{figure}
    \centering
    \includegraphics[width=0.5\linewidth]{grid-dropoff-points.png}
    \caption{Warehouse grid (size 34x32) with dropoff points marked as red squares}
    \label{grid-with-dropoff-points}
\end{figure}

Throughput, defined as the average number of packages delivered per agent over a fixed simulation period (1000 steps per episode), serve as the primary metric for assessing performance. The warehouse environment consistently used for these test was a 34x32 grid, with different scenarios such as the number of shelves, the number of robots, and the number of humans.

\subsection{Performance with no obstacles}
Figure \ref{Empty warehouse increasing robots} shows the relationship between the number of agents and the average throughput per agent in a completely empty warehouse environment. As seen in the graph, the A* algorithm achieved optimal throughput (18 packages per agent) with a single robot. However, as the number of agents increased, the average throughput steadily declined due to increased path congestion and collision avoidance complexity, reaching approximately 13,1 packages per agent at 45 agents.


\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{0_shelves_0humans_increasing_robots.png}
    \caption{Empty warehouse with increasing number of robots}
    \label{Empty warehouse increasing robots}
\end{figure}

\subsection{Performance with human obstacles}
When dynamic human workers were introduced without any shelves 


%%%%%%%%%%%%%%%%%%%%%%%%%%%---DISCUSSION---%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Discussion}




%%%%%%%%%%%%%%%%%%%%%%%%%%%---CONCLUSION---%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Conclusions}


% Print the bibliography (and make it appear in the table of contents)
\printbibliography[heading=bibintoc]

\appendix

\chapter{Something Extra}

% Tailmatter inserts the back cover page (if enabled)
\tailmatter

\end{document}
